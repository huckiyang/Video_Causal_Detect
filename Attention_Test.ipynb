{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code by Sarah Wiegreffe (saw@gatech.edu)\n",
    "# Fall 2019\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "\n",
    "####### Do not modify these imports.\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import math, copy, time\n",
    "class ClassificationTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single-layer Transformer which encodes a sequence of text and \n",
    "    performs binary classification.\n",
    "\n",
    "    The model has a vocab size of V, works on\n",
    "    sequences of length T, has an hidden dimension of H, uses word vectors\n",
    "    also of dimension H, and operates on minibatches of size N.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_to_ix, hidden_dim=128, num_heads=2, dim_feedforward=2048, dim_k=96, dim_v=96, dim_q=96, max_length=43):\n",
    "        '''\n",
    "        :param word_to_ix: dictionary mapping words to unique indices\n",
    "        :param hidden_dim: the dimensionality of the output embeddings that go into the final layer\n",
    "        :param num_heads: the number of Transformer heads to use\n",
    "        :param dim_feedforward: the dimension of the feedforward network model\n",
    "        :param dim_k: the dimensionality of the key vectors\n",
    "        :param dim_q: the dimensionality of the query vectors\n",
    "        :param dim_v: the dimensionality of the value vectors\n",
    "        '''        \n",
    "        super(ClassificationTransformer, self).__init__()\n",
    "        assert hidden_dim % num_heads == 0\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.word_embedding_dim = hidden_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = len(word_to_ix)\n",
    "        \n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    "        self.dim_q = dim_q\n",
    "        \n",
    "        seed_torch(0)\n",
    "        \n",
    "        ##############################################################################\n",
    "        # Deliverable 1: Initialize what you need for the embedding lookup (1 line). #\n",
    "        # Hint: you will need to use the max_length parameter above.                 #\n",
    "        ##############################################################################\n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        # import math\n",
    "        # pe = torch.zeros(max_length, hidden_dim)\n",
    "        # for pos in range(max_length):\n",
    "        #     for i in range(0, hidden_dim, 2):\n",
    "        #         pe[pos, i] = \\\n",
    "        #         math.sin(pos / (10000 ** ((2 * i)/hidden_dim)))\n",
    "        #         pe[pos, i + 1] = \\\n",
    "        #         math.cos(pos / (10000 ** ((2 * (i + 1))/hidden_dim)))\n",
    "                \n",
    "        # pe = pe.unsqueeze(0)\n",
    "#         self.emb_posi = PositionalEncoding(hidden_dim, 0, max_length)\n",
    "        self.emb_posi = nn.Embedding(max_length, hidden_dim)\n",
    "        self.emb_word = nn.Embedding(self.vocab_size, hidden_dim)\n",
    "        # self.embed = PositionalEncoder()\n",
    "        ##############################################################################\n",
    "        #                               END OF YOUR CODE                             #\n",
    "        ##############################################################################\n",
    "        \n",
    "        \n",
    "        ##############################################################################\n",
    "        # Deliverable 2: Initializations for multi-head self-attention.              #\n",
    "        # You don't need to do anything here. Do not modify this code.               #\n",
    "        ##############################################################################\n",
    "        \n",
    "        # Head #1\n",
    "        self.k1 = nn.Linear(self.hidden_dim, self.dim_k)\n",
    "        self.v1 = nn.Linear(self.hidden_dim, self.dim_v)\n",
    "        self.q1 = nn.Linear(self.hidden_dim, self.dim_q)\n",
    "        \n",
    "        # Head #2\n",
    "        self.k2 = nn.Linear(self.hidden_dim, self.dim_k)\n",
    "        self.v2 = nn.Linear(self.hidden_dim, self.dim_v)\n",
    "        self.q2 = nn.Linear(self.hidden_dim, self.dim_q)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.attention_head_projection = nn.Linear(self.dim_v * self.num_heads, self.hidden_dim)\n",
    "        self.norm_mh = nn.LayerNorm(self.hidden_dim)\n",
    "\n",
    "        \n",
    "        ##############################################################################\n",
    "        # Deliverable 3: Initialize what you need for the feed-forward layer.        # \n",
    "        # Don't forget the layer normalization.                                      #\n",
    "        ##############################################################################\n",
    "        self.fc1 = nn.Linear(self.hidden_dim, self.dim_feedforward, bias = True)\n",
    "        self.fc2 = nn.Linear(self.dim_feedforward, self.hidden_dim, bias = True)\n",
    "        self.norm_fc = nn.LayerNorm(self.hidden_dim)\n",
    "        \n",
    "        ##############################################################################\n",
    "        #                               END OF YOUR CODE                             #\n",
    "        ##############################################################################\n",
    "\n",
    "        \n",
    "        ##############################################################################\n",
    "        # Deliverable 4: Initialize what you need for the final layer (1-2 lines).   #\n",
    "        ##############################################################################\n",
    "        self.classify = nn.Linear(self.hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        ##############################################################################\n",
    "        #                               END OF YOUR CODE                             #\n",
    "        ##############################################################################\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        This function computes the full Transformer forward pass.\n",
    "        Put together all of the layers you've developed in the correct order.\n",
    "\n",
    "        :param inputs: a PyTorch tensor of shape (N,T). These are integer lookups. \n",
    "\n",
    "        :returns: the model outputs. Should be normalized scores of shape (N,1).\n",
    "        '''\n",
    "        outputs = None\n",
    "        #############################################################################\n",
    "        # Deliverable 5: Implement the full Transformer stack for the forward pass. #\n",
    "        # You will need to use all of the methods you have previously defined above.#\n",
    "        # You should only be calling ClassificationTransformer class methods here.  #\n",
    "        #############################################################################\n",
    "        x = self.embed(inputs)\n",
    "        x = self.multi_head_attention(x)\n",
    "        x = self.feedforward_layer(x)\n",
    "        outputs = self.final_layer(x)\n",
    "        \n",
    "        ##############################################################################\n",
    "        #                               END OF YOUR CODE                             #\n",
    "        ##############################################################################\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def embed(self, inputs):\n",
    "        \"\"\"\n",
    "        :param inputs: intTensor of shape (N,T)\n",
    "        :returns embeddings: floatTensor of shape (N,T,H)\n",
    "        \"\"\"\n",
    "        embeddings = None\n",
    "        #############################################################################\n",
    "        # Deliverable 1: Implement the embedding lookup.                            #\n",
    "        # Note: word_to_ix has keys from 0 to self.vocab_size - 1                   #\n",
    "        # This will take a few lines.                                               #\n",
    "        #############################################################################\n",
    "        # N_size = inputs.size()[0]\n",
    "        # T_size = inputs.size()[1]\n",
    "        N_size, T_size = inputs.shape\n",
    "        tmp_embd = torch.zeros(N_size, T_size, self.hidden_dim)\n",
    "        # for i in range(T_size):\n",
    "        #     tmp_embd[:, i, :] += self.emb_word(inputs[:, i])\n",
    "        #     tmp_embd[:, i, :] += self.emb_posi(i * torch.ones(N_size, dtype = torch.long))\n",
    "\n",
    "        embeddings = tmp_embd\n",
    "        ##############################################################################\n",
    "        #                               END OF YOUR CODE                             #\n",
    "        ##############################################################################\n",
    "        return embeddings\n",
    "        \n",
    "    def multi_head_attention(self, inputs):\n",
    "        \"\"\"\n",
    "        :param inputs: float32 Tensor of shape (N,T,H)\n",
    "        :returns outputs: float32 Tensor of shape (N,T,H)\n",
    "        \n",
    "        Traditionally we'd include a padding mask here, so that pads are ignored.\n",
    "        This is a simplified implementation.\n",
    "        \"\"\"\n",
    "        \n",
    "        outputs = None\n",
    "        #############################################################################\n",
    "        # Deliverable 2: Implement multi-head self-attention followed by add + norm.#\n",
    "        # Use the provided 'Deliverable 2' layers initialized in the constructor.   #\n",
    "        #############################################################################\n",
    "        \n",
    "        N = inputs.size()[0]\n",
    "        T = inputs.size()[1]\n",
    "\n",
    "        Q1 = self.q1(inputs)\n",
    "        K1 = self.k1(inputs)\n",
    "        V1 = self.v1(inputs)\n",
    "\n",
    "        Q2 = self.q2(inputs)\n",
    "        K2 = self.k2(inputs)\n",
    "        V2 = self.v2(inputs)\n",
    "\n",
    "\n",
    "        mat1 = self.softmax( torch.bmm(Q1, K1.transpose(1,2)) / math.sqrt(self.dim_k))\n",
    "        attention1 = torch.matmul(mat1, V1)\n",
    "\n",
    "        mat2 = self.softmax( torch.bmm(Q2, K2.transpose(1,2) ) / math.sqrt(self.dim_k))\n",
    "        attention2 = torch.matmul(mat2, V2)\n",
    "\n",
    "\n",
    "        attention = torch.cat((attention1, attention2), dim = 2)\n",
    "        sublayer = self.attention_head_projection(attention)\n",
    "        outputs = self.norm_mh(inputs + sublayer)\n",
    "        ##############################################################################\n",
    "        #                               END OF YOUR CODE                             #\n",
    "        ##############################################################################\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def feedforward_layer(self, inputs):\n",
    "        \"\"\"\n",
    "        :param inputs: float32 Tensor of shape (N,T,H)\n",
    "        :returns outputs: float32 Tensor of shape (N,T,H)\n",
    "        \"\"\"\n",
    "        outputs = None\n",
    "        #############################################################################\n",
    "        # Deliverable 3: Implement the feedforward layer followed by add + norm.    #\n",
    "        # Use a ReLU activation and apply the linear layers in the order you        #\n",
    "        # initialized them.                                                         #\n",
    "        # This should not take more than 3-5 lines of code.                         #\n",
    "        #############################################################################\n",
    "        x = self.fc1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        outputs = self.norm_fc(inputs + x)\n",
    "        \n",
    "        ##############################################################################\n",
    "        #                               END OF YOUR CODE                             #\n",
    "        ##############################################################################\n",
    "        return outputs\n",
    "        \n",
    "    \n",
    "    def final_layer(self, inputs):\n",
    "        \"\"\"\n",
    "        :param inputs: float32 Tensor of shape (N,T,H)\n",
    "        :returns outputs: float32 Tensor of shape (N,1)\n",
    "        \"\"\"\n",
    "        outputs = None\n",
    "        #############################################################################\n",
    "        # Deliverable 4: Implement the final layer for the Transformer classifier.  #\n",
    "        # This should not take more than 2 lines of code.                         #\n",
    "        #############################################################################\n",
    "        x = self.classify(inputs[:, 0, :])\n",
    "        outputs = self.sigmoid(x)\n",
    "        \n",
    "        ##############################################################################\n",
    "        #                               END OF YOUR CODE                             #\n",
    "        ##############################################################################\n",
    "        return outputs\n",
    "        \n",
    "\n",
    "def seed_torch(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1542\n",
      "(7000, 43)\n",
      "(1551, 43)\n",
      "(7000,)\n",
      "(1551,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "\n",
    "train_inxs = np.load('./gt_7643/datasets/train_inxs.npy')\n",
    "val_inxs = np.load('./gt_7643/datasets/val_inxs.npy')\n",
    "train_labels = np.load('./gt_7643/datasets/train_labels.npy')\n",
    "val_labels = np.load('./gt_7643/datasets/val_labels.npy')\n",
    "\n",
    "# load dictionary\n",
    "word_to_ix = {}\n",
    "with open(\"./gt_7643/datasets/word_to_ix.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        word_to_ix[line[0]] = line[1]\n",
    "print(\"Vocabulary Size:\", len(word_to_ix))\n",
    "        \n",
    "print(train_inxs.shape) # 7000 training instances, of (maximum/padded) length 43 words.\n",
    "print(val_inxs.shape) # 1551 validation instances, of (maximum/padded) length 43 words.\n",
    "print(train_labels.shape)\n",
    "print(val_labels.shape)\n",
    "\n",
    "# load checkers\n",
    "d1 = torch.load('./gt_7643/datasets/d1.pt')\n",
    "d2 = torch.load('./gt_7643/datasets/d2.pt')\n",
    "d3 = torch.load('./gt_7643/datasets/d3.pt')\n",
    "d4 = torch.load('./gt_7643/datasets/d4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: 2368.49365234375\n"
     ]
    }
   ],
   "source": [
    "inputs = train_inxs[0:2]\n",
    "inputs = torch.LongTensor(inputs)\n",
    "\n",
    "model = ClassificationTransformer(word_to_ix, hidden_dim=128, num_heads=2, dim_feedforward=2048, dim_k=96, \n",
    "                                  dim_v=96, dim_q=96, max_length=train_inxs.shape[1])\n",
    "\n",
    "embeds = model.embed(inputs)\n",
    "\n",
    "try:\n",
    "    print(\"Difference:\", torch.sum(torch.pairwise_distance(embeds, d1)).item()) # should be very small (<0.01)\n",
    "except:\n",
    "    print(\"NOT IMPLEMENTED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10,   12,    0,    0,   13,   14,    0,   15, 1540, 1541, 1541, 1541,\n",
       "        1541, 1541, 1541, 1541, 1541, 1541, 1541, 1541, 1541, 1541, 1541, 1541,\n",
       "        1541, 1541, 1541, 1541, 1541, 1541, 1541, 1541, 1541, 1541, 1541, 1541,\n",
       "        1541, 1541, 1541, 1541, 1541, 1541, 1541])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "    \n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=43):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder_2(nn.Module):\n",
    "    def __init__(self, max_seq_len = 43, d_model=128):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # make embeddings relatively larger\n",
    "        inputs = inputs * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = inputs.size(1)\n",
    "        inputs = inputs + Variable(self.pe[:,:seq_len], \\\n",
    "        requires_grad=False)\n",
    "        return inputs\n",
    "    \n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 43)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 43, 128])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding(128,0 )\n",
    "y = pe.forward(d1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = train_inxs[0:2]\n",
    "inputs = torch.LongTensor(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, T = inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_embd = torch.zeros(N, T, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "posi = nn.Embedding(43, 128)\n",
    "word = nn.Embedding(len(word_to_ix), 128)\n",
    "posi_2 = PositionalEncoder_2(43,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: 17768.669921875\n"
     ]
    }
   ],
   "source": [
    "inputs = train_inxs[0:2]\n",
    "inputs = torch.LongTensor(inputs)\n",
    "\n",
    "model = ClassificationTransformer(word_to_ix, hidden_dim=128, num_heads=2, dim_feedforward=2048, dim_k=96, \n",
    "                                  dim_v=96, dim_q=96, max_length=train_inxs.shape[1])\n",
    "\n",
    "embeds = posi_2(word(inputs))\n",
    "\n",
    "try:\n",
    "    print(\"Difference:\", torch.sum(torch.pairwise_distance(embeds, d1)).item()) # should be very small (<0.01)\n",
    "except:\n",
    "    print(\"NOT IMPLEMENTED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = np.ones((3,3,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5.,  6.,  7.],\n",
       "        [ 6.,  7.,  8.],\n",
       "        [ 7.,  8.,  9.]],\n",
       "\n",
       "       [[ 7.,  8.,  9.],\n",
       "        [ 9., 10., 11.],\n",
       "        [11., 12., 13.]],\n",
       "\n",
       "       [[ 9., 10., 11.],\n",
       "        [12., 13., 14.],\n",
       "        [15., 16., 17.]]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        for k in range(3):\n",
    "            ok[i][j][k] = (i+1)*(j+2)+(k+3)\n",
    "    \n",
    "ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 6., 7.],\n",
       "       [6., 7., 8.],\n",
       "       [7., 8., 9.]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ok[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs7643] *",
   "language": "python",
   "name": "conda-env-cs7643-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
